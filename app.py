import streamlit as st
import numpy as np
import json
import pandas as pd
import aspose.words as aw
import re
import pickle
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from stop_words import get_stop_words
from pymystem3 import Mystem
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–ª–µ–º–µ–Ω—Ç –∫–ª–∞—Å—Å–∞ Mystem() –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏  —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
m = Mystem() 

# –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ —Å–ø–∏—Å–∫–µ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ –¥–æ–≥–æ–≤–æ—Ä–∞
MAX_WORD = 1000 # –ø–æ–¥–±–∏—Ä–∞–µ—Ç—Å—è –Ω–∞–∏–ª—É—á—à–∏–π –ø–æ –º–µ—Ç—Ä–∏–∫–µ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–∏–¥–∞—Ü–∏–∏ 

STOPWORDS_RU = get_stop_words('russian')

# —Å–ª–æ–≤–∞—Ä—å –≤–∏–¥–æ–≤ –¥–æ–≥–æ–≤–æ—Ä–æ–≤
with open('data/kind_names.json', 'r', encoding='utf8') as f:
    kind_names = json.load(f)

with open('data/pkl_object/vectorizer.pkl', 'rb') as f:
    vectorizer = pickle.load(f)
    
with open('data/pkl_object/logit_grid_searcher.pkl', 'rb') as f:
    logit_grid_searcher = pickle.load(f)
         
@st.cache(allow_output_mutation=True)
# —Ñ—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞
def GetTextContract(path):
    
    data = aw.Document(path)
    contract = data.get_text()
    contract = contract.replace('Evaluation Only. Created with Aspose.Words. Copyright 2003-2022 Aspose Pty Ltd.', '')
    contract = contract.replace('Created with an evaluation copy of Aspose.Words. To discover the full versions of our APIs please visit: https://products.aspose.com/words/', '')  
    
    return contract

# —Ñ—É–Ω–∫—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
def ContractTransform(contract):

    contract = contract.lower()
    contract = re.sub('[^–∞-—è–ê-–Ø—ë–Å]', ' ', contract)
    lemm_text_list = [i for i in m.lemmatize(contract) if len(i.strip()) > 2]
    lemm_text_list = [word for word in lemm_text_list if word != '–¥–æ–≥–æ–≤–æ—Ä']
    lemm_text_list = [word for word in lemm_text_list if '—Å—Ç–æ—Ä–æ–Ω' not in word]
    lemm_text_list = lemm_text_list[:MAX_WORD]
    
    return pd.Series(' '.join(lemm_text_list))

# —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ–±–ª–∞–∫–∞ —Å–ª–æ–≤
def CreateWordCloud(text):
    
    wordcloud = WordCloud(width = 3000, 
                      height = 2000, 
                      random_state=42,
                      background_color='white',
                      repeat = True,
                      collocations=False,
                      stopwords = STOPWORDS_RU).generate(text) 
                        
    return wordcloud

# —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
def ModelPredictProba(contract, kind_names, vectorizer, logit_grid_searcher):
    
    X = vectorizer.transform(contract)
    kind_name_pred = kind_names[str(logit_grid_searcher.predict(X)[0])]
        
    return kind_name_pred
    
st.set_page_config(
    page_title="DocTypes", page_icon="‚ùÑÔ∏è", initial_sidebar_state="expanded"
)

st.write(
    """
# üìÉ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–≥–æ–≤–æ—Ä–æ–≤

–ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Ñ–∞–π–ª—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è **.DOC**, **.RTF**, **.PDF**, **.DOCX**

"""
)

st.write('**–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–≥–æ–≤–æ—Ä–∞**')

uploaded_file = st.file_uploader(label='–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª:',
                            type=["doc", "docx", "pdf", "rtf"], accept_multiple_files=False,
                            help='—É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –ø–µ—Ä–µ—Ç–∞—â–∏—Ç–µ –µ–≥–æ –≤ –æ–Ω–∫–æ –∑–∞–≥—Ä—É–∑–∫–∏')
if uploaded_file:
    contract = GetTextContract(uploaded_file)
    contract_name = uploaded_file.name                    
    st.success("–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω", icon="‚úÖ")
    st.components.v1.html(contract, width=None, height=300, scrolling=True)
    result = st.button('–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å')
    if result:
        contract = ContractTransform(contract)
        kind_name_pred = ModelPredictProba(contract, kind_names, vectorizer, logit_grid_searcher)
        st.success("–í–∏–¥ –¥–æ–≥–æ–≤–æ—Ä–∞ —É—Å–ø–µ—à–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω", icon="‚úÖ")
        
        st.write(f"""**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
        
    üìå –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞: {contract_name}
        
    ‚úîÔ∏è –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –≤–∏–¥ –¥–æ–≥–æ–≤–æ—Ä–∞:  {kind_name_pred}
       
        """)
    # –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        st.write('**–ü–æ—á–µ–º—É –∞–ª–≥–æ—Ä–∏—Ç–º —Ç–∞–∫ —Å—á–∏—Ç–∞–µ—Ç?**') 
        wordcloud = CreateWordCloud(contract[0])
    
        fig, ax = plt.subplots(figsize = (12, 8))
        ax.imshow(wordcloud)
        plt.axis("off")
        st.pyplot(fig)
else:
    st.error("–í—ã –Ω–∏—á–µ–≥–æ –Ω–µ –≤–∞–±—Ä–∞–ª–∏", icon="‚ùå")
    
st.markdown("<h5 style='text-align: center; color: blac;'> ¬©Ô∏è –ö–æ–º–∞–Ω–¥–∞ 40+ </h5>", unsafe_allow_html=True)
st.markdown("<h5 style='text-align: center; color: blac;'> X-MAS HACK 2022 </h5>", unsafe_allow_html=True)